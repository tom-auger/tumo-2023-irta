{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bef76c6",
   "metadata": {},
   "source": [
    "# Lab 3: Text Classification\n",
    "\n",
    "Text classification is the process of sorting documents of text into a set of predefined categories. \n",
    "\n",
    "In this lab we'll train machine learning models to classify the sentiment of Twitter tweets as either **positive, negative, or neutral**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49dae194",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from collections import Counter\n",
    "import re\n",
    "import string\n",
    "from scipy import sparse \n",
    "import sklearn\n",
    "from sklearn import svm, ensemble, naive_bayes, linear_model\n",
    "import numpy as np\n",
    "import Stemmer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbca7bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The file 'datasets/stopwords_en.txt' contains a list of stopwords - one per line.\n",
    "with open(\"datasets/stopwords_en.txt\") as f:\n",
    "    enStopWords = set(f.read().splitlines())\n",
    "\n",
    "# Initialze the SnowballStemmer\n",
    "enStemmer = Stemmer.Stemmer('english')\n",
    "\n",
    "def preprocess_line_en(line: str) -> list[str]:\n",
    "    # Convert to lower case\n",
    "    tokens = line.lower()\n",
    "    \n",
    "    # Split into tokens with no punctuation\n",
    "    tokens = re.split(\"[^\\w]\", tokens)\n",
    "    \n",
    "    # Remove empty strings and stop words and apply the stemmer\n",
    "    # Initially disable stopping and stemming\n",
    "    #tokens = [enStemmer.stemWord(x) for x in tokens if x and x not in enStopWords]\n",
    "    \n",
    "    # Return the tokens\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa3a0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(lines):\n",
    "    # Preprocess data for classification\n",
    "    documents = []\n",
    "    categories = []\n",
    "    vocab = set()\n",
    "    \n",
    "    # url_regex = re.compile(r'https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)')\n",
    "        \n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            tweet_id, category, tweet = line.split('\\t')\n",
    "\n",
    "            # tweet = url_regex.sub('', tweet)\n",
    "                \n",
    "            # Preprocess the text\n",
    "            terms = preprocess_line_en(tweet)\n",
    "\n",
    "            # Add terms to the vocab\n",
    "            for term in terms:\n",
    "                vocab.add(term)\n",
    "           \n",
    "            # Add document to the collection of docs\n",
    "            documents.append(terms)\n",
    "\n",
    "            # Add category to the list of categories\n",
    "            categories.append(category)\n",
    "    \n",
    "    return documents, categories, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9280b70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset(train_test):\n",
    "    with open(f\"datasets/tweets_{train_test}.txt\", encoding=\"latin-1\") as f:\n",
    "        # Skip the header \n",
    "        return f.readlines()[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c0379e",
   "metadata": {},
   "source": [
    "Load the dataset, splitting it into a 90-10% train/validation test set split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effc45fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_val_data(seed):\n",
    "    # Load the corpus\n",
    "\n",
    "    # Randomly permute the corpus\n",
    "    \n",
    "    # Split the data into 90% training set 10% development set\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2b1425",
   "metadata": {},
   "source": [
    "1. Print the number of documents and size of vocabulary in each dataset.\n",
    "2. Print the distribution of categories in the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eead702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print some info about the dataset\n",
    "\n",
    "\n",
    "# Print the distribution of categories in the training set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66133ca0",
   "metadata": {},
   "source": [
    "Convert the dataset into a bag-of-words representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef70c13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Every word in the vocabulary is given an index\n",
    "\n",
    "\n",
    "# Every category is given an index too\n",
    "\n",
    "\n",
    "# Convert to bag of words\n",
    "# This is a count matrix where each row is a document and each column is a word. \n",
    "# The elements of the matrix are the frequencies of how often that word appears in the dataset.\n",
    "def convert_to_bow_matrix(data, word2id):\n",
    "\n",
    "    # Matrix size is number of docs, vocab size + 1 (for OOV)\n",
    "\n",
    "    # Matrix indexed by [doc_id, token_id]\n",
    "    \n",
    "    # Loop over documents and insert word counts\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47bea3bc",
   "metadata": {},
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7aefb01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f0e03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Â Answer\n",
    "X_train = convert_to_bow_matrix(train_docs, word2id)\n",
    "y_train = [cat2id[cat] for cat in train_categories]\n",
    "\n",
    "# Train the model\n",
    "model = sklearn.svm.SVC(C=1000)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fc280d",
   "metadata": {},
   "source": [
    "Compute the accuracy of the model by evaluating predictions on the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1713883f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(predictions, targets):\n",
    "    num_correct = 0\n",
    "    num_total = len(predictions)\n",
    "    for predicted, target in zip(predictions, targets):\n",
    "        if predicted == target:\n",
    "            num_correct += 1\n",
    "    return num_correct / num_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f743b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_predictions = model.predict(X_train)\n",
    "accuracy = compute_accuracy(y_train_predictions, y_train)\n",
    "print(\"Training accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abd0c94",
   "metadata": {},
   "source": [
    "Compute the accuracy of the model on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd124cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy on the validation set\n",
    "X_val = convert_to_bow_matrix(val_docs, word2id)\n",
    "y_val = [cat2id[cat] for cat in val_categories]\n",
    "\n",
    "y_val_predictions = model.predict(X_val)\n",
    "accuracy = compute_accuracy(y_val_predictions, y_val)\n",
    "\n",
    "print(\"Validation accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0222126c",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "Can we improve the performance of the model? Try some of the following ideas and experiment with any of your own:\n",
    "1. Modify preprocessing to remove hyperlinks\n",
    "2. Modify preprocessing to keep hashtags (don't remove the # character)\n",
    "3. Try using another model instead of SVM, for example `sklearn.linear_model.LogisticRegression` or `sklearn.ensemble.RandomForestClassifier`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df894969",
   "metadata": {},
   "source": [
    "Once all optimizations to the model have been made evaluate it on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e782a6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and calculate accuracy on the test set\n",
    "# X_test = convert_to_bow_matrix(test_docs, word2id)\n",
    "# y_test = [cat2id[cat] for cat in test_categories]\n",
    "\n",
    "# y_test_predictions = model.predict(X_test)\n",
    "# accuracy = compute_accuracy(y_test_predictions, y_test)\n",
    "# print(\"Test accuracy: \", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
