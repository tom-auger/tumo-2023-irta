{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc8f47ea",
   "metadata": {},
   "source": [
    "# Lesson 2: Themes and Topics\n",
    "\n",
    "Given two documents how can we determine what they are about? In this lab we will explore two methods for doing this.\n",
    "\n",
    "The first uses statistics such as mutual information and chi-squared to determine unique words. \n",
    "\n",
    "The second uses a more advanced method called Latent Dirichlet Algorithm or LDA to determine which topics are present in the document."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295e2de0",
   "metadata": {},
   "source": [
    "## Corpus\n",
    "\n",
    "The dataset is a text file with two columns. The first column contains one of three document names:\n",
    "* \"Quran\"\n",
    "* \"OT\"\n",
    "* \"NT\"\n",
    "\n",
    "where OT is Old Testament and NT is New Testament.\n",
    "The second column contains the document text, in this case a verse from one of the three document names. \n",
    "\n",
    "**The document name and the document are separated by a tab character '\\t'**.\n",
    "\n",
    "Run the following code to output the first five lines of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e5c3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"datasets/comparing_train_and_dev.tsv\") as f:\n",
    "    for i in range(5):\n",
    "        print(f.readline())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc354f3",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8674aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import Stemmer\n",
    "import os\n",
    "from collections import Counter\n",
    "import unittest\n",
    "%reload_ext ipython_unittest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8f88b9",
   "metadata": {},
   "source": [
    "The cell below contains the preprocessing code for English text from Lab 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1087ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The file 'datasets/stopwords_en.txt' contains a list of stopwords - one per line.\n",
    "with open(\"datasets/stopwords_en.txt\") as f:\n",
    "    enStopWords = set(f.read().splitlines())\n",
    "\n",
    "#      stopWords.extend(['thy', 'ye', 'thou', 'thee', 'shalt', 'hath'])\n",
    "\n",
    "# Initialze the SnowballStemmer\n",
    "enStemmer = Stemmer.Stemmer('english')\n",
    "\n",
    "def preprocess_line_en(line: str) -> list[str]:\n",
    "    # Convert to lower case\n",
    "    tokens = line.lower()\n",
    "    \n",
    "    # Split into tokens with no punctuation\n",
    "    tokens = re.split(\"[^\\w]\", tokens)\n",
    "    \n",
    "    # Remove empty strings and stop words and apply the stemmer\n",
    "    tokens = [enStemmer.stemWord(x) for x in tokens if x and x not in enStopWords]\n",
    "    \n",
    "    # Return the tokens\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae64b738",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "Given a doc_name \"Quran\", \"NT\", or \"OT\" and implement the following function that reads each line of the corpus and preprocesses the selected documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42223a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(doc_name: str, dataset: list[str]) -> list[str]:\n",
    "    \"\"\"Preprocesses and returns the documents from the dataset with the given doc name\"\"\"\n",
    "    \n",
    "    # 1. Loop over each line in the dataset and split it on the tab character '\\t'. \n",
    "    # The first part is the document name the second part is the document.\n",
    "    \n",
    "    # 2. If the document name is the one specified in the parameter doc_name then preprocess it using \n",
    "    # preprocess_line_en.\n",
    "    \n",
    "    # 3. Return all the preprocessed documents.\n",
    "    # Example return value: [['owner', 'day', 'recompens'], ['guid', 'straight', 'path'], ...]\n",
    "    \n",
    "    raise Exception(\"Implement me!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5211e6",
   "metadata": {},
   "source": [
    "Run the following test to verify your preprocessing function works as expected. If it passes you should see it succeeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8537dd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_corpus = ['Quran\\tPraise be to Allah, Lord of the Worlds,',\n",
    " 'Quran\\tthe Merciful, the Most Merciful,',\n",
    " 'Quran\\tOwner of the Day of Recompense.',\n",
    " 'Quran\\tGuide us to the Straight Path,',\n",
    " 'NT\\tNow the birth of Jesus Christ was on this wise: When as his mother Mary was espoused to Joseph, before they came together, she was found with child of the Holy Ghost.',\n",
    " 'NT\\tThen Joseph her husband, being a just man, and not willing to make her a publick example, was minded to put her away privily.',\n",
    " 'NT\\tBut while he thought on these things, behold, the angel of the LORD appeared unto him in a dream, saying, Joseph, thou son of David, fear not to take unto thee Mary thy wife: for that which is conceived in her is of the Holy Ghost.',\n",
    " 'NT\\tAnd she shall bring forth a son, and thou shalt call his name JESUS: for he shall save his people from their sins.',\n",
    " 'OT\\tAnd the earth was without form, and void; and darkness was upon the face of the deep. And the Spirit of God moved upon the face of the waters.',\n",
    " 'OT\\tAnd God said, Let there be light: and there was light.',\n",
    " 'OT\\tAnd God saw the light, that it was good: and God divided the light from the darkness.',\n",
    " 'OT\\tAnd God called the light Day, and the darkness he called Night. And the evening and the morning were the first day.',\n",
    " 'OT\\tAnd God said, Let there be a firmament in the midst of the waters, and let it divide the waters from the waters.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6157e62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%unittest\n",
    "\"Quran documents are correctly processed\"\n",
    "result = preprocess_dataset(\"Quran\", test_corpus)\n",
    "assert result == [['prais', 'allah', 'lord', 'world'], ['merci', 'merci'], ['owner', 'day', 'recompens'], ['guid', 'straight', 'path']]\n",
    "\n",
    "\"Old Testament documents are correctly processed\"\n",
    "result = preprocess_dataset(\"OT\", test_corpus)\n",
    "assert result == [['earth', 'form', 'void', 'dark', 'face', 'deep', 'spirit', 'god', 'move', 'face', 'water'], ['god', 'light', 'light'], ['god', 'light', 'good', 'god', 'divid', 'light', 'dark'], ['god', 'call', 'light', 'day', 'dark', 'call', 'night', 'even', 'morn', 'day'], ['god', 'firmament', 'midst', 'water', 'divid', 'water', 'water']]\n",
    "\n",
    "\"New Testament documents are correctly processed\"\n",
    "result = preprocess_dataset(\"NT\", test_corpus)\n",
    "assert result == [['birth', 'jesus', 'christ', 'wise', 'mother', 'mari', 'espous', 'joseph', 'found', 'child', 'holi', 'ghost'], ['joseph', 'husband', 'man', 'make', 'publick', 'mind', 'put', 'privili'], ['thought', 'thing', 'behold', 'angel', 'lord', 'appear', 'dream', 'joseph', 'thou', 'son', 'david', 'fear', 'thee', 'mari', 'thi', 'wife', 'conceiv', 'holi', 'ghost'], ['bring', 'son', 'thou', 'shalt', 'call', 'jesus', 'save', 'peopl', 'sin']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20994101",
   "metadata": {},
   "source": [
    "## Mutual Information & Chi Squared\n",
    "\n",
    "The following functions in the cell below calculate mutual information and chi squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77188a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda778e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_term_doc_freqs(corpus):\n",
    "    # For each term find the number of docs in the corpus that contain it\n",
    "    freqs = Counter()\n",
    "\n",
    "    for doc in corpus:\n",
    "        terms = set(doc)\n",
    "        freqs.update(terms)\n",
    "        \n",
    "    return freqs \n",
    "\n",
    "def xlogy(x, y):\n",
    "    # Compute x * log(y). If x == 0 then return zero without\n",
    "    # evaluating log(y)\n",
    "    if x == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return x * log2(y)\n",
    "\n",
    "def compute_mi(n00, n10, n01, n11):\n",
    "    # Compute mutual information\n",
    "    N = n00 + n10 + n01 + n11\n",
    "    return xlogy((n11 / N), (N * n11) / ((n11 + n10) * (n11 + n01))) + \\\n",
    "            xlogy((n01 / N), (N * n01) / ((n01 + n00) * (n11 + n01))) + \\\n",
    "            xlogy((n10 / N), (N * n10) / ((n11 + n10) * (n10 + n00))) + \\\n",
    "            xlogy((n00 / N), (N * n00) / ((n01 + n00) * (n10 + n00)))\n",
    "\n",
    "def compute_chi_sq(n00, n10, n01, n11):\n",
    "    # Compute chi squared\n",
    "    return ((n11 + n10 + n01 + n00) * (n11 * n00 - n10 * n01)**2) / \\\n",
    "            ((n11 + n01) * (n11 + n10) * (n10 + n00) * (n01 + n00))\n",
    "\n",
    "def compute_mi_and_chi_sq(corpus, other_corpus, vocab):\n",
    "    # Compute the term doc frequencies for the corpus and the other corpus\n",
    "    corpus_freqs = compute_term_doc_freqs(corpus)\n",
    "    other_corpus_freqs = compute_term_doc_freqs(other_corpus)\n",
    "\n",
    "    # For each term in the vocab compute the MI and Chi squared\n",
    "    mut_inf = {}\n",
    "    chi_sq = {}\n",
    "\n",
    "    for term in vocab:\n",
    "\n",
    "        # Number of docs in corpus containing term\n",
    "        n11 = corpus_freqs.get(term, 0)\n",
    "\n",
    "        # Number of docs in corpus not containing term\n",
    "        n10 = len(corpus) - n11 \n",
    "\n",
    "        # Number of docs not in corpus containing term\n",
    "        n01 = other_corpus_freqs.get(term, 0)\n",
    "\n",
    "        # Number of docs not in corpus not containing term\n",
    "        n00 = len(other_corpus) - n01 \n",
    "\n",
    "        # Compute MI\n",
    "        mut_inf[term] = compute_mi(n00, n10, n01, n11)\n",
    "\n",
    "        # Compute Chi squared\n",
    "        chi_sq[term] = compute_chi_sq(n00, n10, n01, n11)\n",
    "    \n",
    "    return mut_inf, chi_sq "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5189d4b2",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "Write code following the steps below to calculate mutual information and chi squared for the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22d092a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus() -> list[str]:\n",
    "    with open(\"datasets/comparing_train_and_dev.tsv\") as f:\n",
    "        return f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33408922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Read the corpus by calling the function read_corpus defined above. \n",
    "\n",
    "# 2. Preprocess the datasets \"Quran\", \"OT\", and \"NT\" using the preprocess_dataset function.\n",
    "\n",
    "# 3. Generate the vocabulary (a list of all the unique words) in the entire preprocessed corpus.\n",
    "\n",
    "# 4. By calling the function compute_mi_and_chi_sq calculate the mutual information \n",
    "# and chi squared values for the Quran. The parameter corpus should be the preprocessed Quran documents and\n",
    "# the 'other_corpus' is the combination of the preprocessed old and new testaments.\n",
    "\n",
    "# 5. Sort the statistics from highest to lowest and print the top 10 mutual information words \n",
    "# and the top 10 chi squared words for the Quran.\n",
    "\n",
    "# 6. Repeat steps 4 and 5 to calculate the mutual information and chi squared for the Old Testament and \n",
    "# the New Testament.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7199c3",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "1. What differences do you observe between the top Mutual Information words and the top Chi Squared words?\n",
    "\n",
    "2. What can you learn about the three documents, the Quran, Old Testament, and New Testament from these results?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2a37ba",
   "metadata": {},
   "source": [
    "## Extension exercises\n",
    "\n",
    "1. Notice that words like \"thou\", \"thi\", \"ye\" appear in the output - what do you think these words mean in English?\n",
    "2. It turns out these are old English words for \"you\" and \"they\" - but these should be stop words! Modify the preprocessing code to exclude these words.\n",
    "3. Recalculate the Mutual Information and Chi Squared scores. Do you get better results?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7ffda4",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3584855",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gensim import models\n",
    "from gensim.corpora.dictionary import Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831ad48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = read_corpus()\n",
    "quran = preprocess_dataset(\"Quran\", corpus)\n",
    "new_testament = preprocess_dataset(\"NT\", corpus)\n",
    "old_testament = preprocess_dataset(\"OT\", corpus)\n",
    "\n",
    "# Combine all the preprocessed corpora together\n",
    "common_texts = quran + new_testament + old_testament\n",
    "\n",
    "# Create dictionary \n",
    "corpus_dictionary = Dictionary(common_texts)\n",
    "corpus = [corpus_dictionary.doc2bow(d) for d in common_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc981f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the LDA model with 20 topics\n",
    "lda = models.LdaModel(corpus, num_topics=20, id2word=corpus_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a88a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the top 5 docs from the quran\n",
    "for i in range(5):\n",
    "    doc_bow = corpus_dictionary.doc2bow(quran[i])\n",
    "    topics = lda.get_document_topics(doc_bow)\n",
    "    print('Document: \\n', quran[i])\n",
    "    print('Topics: \\n', topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9dc8bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the top 5 words for each topic\n",
    "print(\"The top five words for each topic:\")\n",
    "topics = lda.print_topics(num_words=5)\n",
    "for topic_id, words in topics:\n",
    "    print(topic_id, ': ', words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a66454f",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "\n",
    "In this exercise we'll write some code to interpret the topics identified by the LDA model.\n",
    "\n",
    "1. For each of the three corpora: Quran, Old Testament, and New Testament, compute the average score for each topic by summing the document-topic probability for each document in that corpus and dividing by the total number of documents in the corpus.\n",
    "2. Then for each corpus, identify the top topic that has the highest average score. For each of those three top topics print the top 10 tokens with highest probability of belonging to that topic. Hint: use `lda.print_topic`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7dd824",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "1. For each of the three top topics what title in 1-3 words would you give to that topic?\n",
    "\n",
    "2. What does the LDA model tell you about the corpus? Consider the top three topics you have found. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac4c02d",
   "metadata": {},
   "source": [
    "### Extension Exercises\n",
    "\n",
    "1. Print out a list of all the topics for each corpus. Are there any topics that appear to be common in 2 corpora but not in the other? What are they and what are some examples of high probability words from these topics?\n",
    "\n",
    "2. How do these results differ from what you learned using mutual information and chi squared?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
