{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb3c143d",
   "metadata": {},
   "source": [
    "# Lesson 4: Search Index and Boolean Search\n",
    "\n",
    "In this lesson we'll learn to how build a search index and make basic queries against it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "606bc65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "import os\n",
    "import math\n",
    "import Stemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56239331",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "This is the same preprocessing that we've been doing up till now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4abe265d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The file 'datasets/stopwords_en.txt' contains a list of stopwords - one per line.\n",
    "with open(\"../datasets/stopwords_en.txt\") as f:\n",
    "    enStopWords = set(f.read().splitlines())\n",
    "\n",
    "# Initialze the SnowballStemmer\n",
    "enStemmer = Stemmer.Stemmer('english')\n",
    "\n",
    "def preprocess_line_en(line: str) -> list[str]:\n",
    "    # Convert to lower case\n",
    "    tokens = line.lower()\n",
    "    \n",
    "    # Split into tokens with no punctuation\n",
    "    tokens = re.split(\"[^\\w]\", tokens)\n",
    "    \n",
    "    # Remove empty strings and stop words and apply the stemmer\n",
    "    tokens = [enStemmer.stemWord(x) for x in tokens if x and x not in enStopWords]\n",
    "    \n",
    "    # Return the tokens\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20917db7",
   "metadata": {},
   "source": [
    "## Building the Index\n",
    "\n",
    "Let's build a basic index which for every word in the collection contains the indexes of the documents that contain it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3fcf3d9",
   "metadata": {},
   "source": [
    "The collection of documents we will index at first is `datasets/example_search_dataset.xml` one:\n",
    "```\n",
    "<document>\n",
    "<DOC>\n",
    "\t<DOCNO>1</DOCNO>\n",
    "\t<Text>\n",
    "\t\tHe likes to wink, he likes to drink\n",
    "\t</Text>\n",
    "</DOC>\n",
    "\t\n",
    "<DOC>\n",
    "\t<DOCNO>2</DOCNO>\n",
    "\t<Text>\n",
    "\t\tHe likes to drink, and drink, and drink\n",
    "\t</Text>\n",
    "</DOC>\n",
    "\n",
    "<DOC>\n",
    "\t<DOCNO>3</DOCNO>\n",
    "\t<Text>\n",
    "\t\tThe thing he likes to drink is ink\n",
    "\t</Text>\n",
    "</DOC>\n",
    "\n",
    "<DOC>\n",
    "\t<DOCNO>4</DOCNO>\n",
    "\t<Text>\n",
    "\t\tThe ink he likes to drink is pink\n",
    "\t</Text>\n",
    "</DOC>\n",
    "\n",
    "<DOC>\n",
    "\t<DOCNO>5</DOCNO>\n",
    "\t<Text>\n",
    "\t\tHe likes to wink, and drink pink ink\n",
    "\t</Text>\n",
    "</DOC>\n",
    "</document>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1f22f2",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Create an inverted index for the above collection of documents in `example_search_dataset.xml`.\n",
    "\n",
    "You need to save the following information:\n",
    "- The term (pre-processed) and its document frequency (optional)\n",
    "- list of documents where this term occured\n",
    "- for each document, list of positions where the term occured within the document, and the number of term positions (optional)\n",
    "\n",
    "Print out your index to visualise it.\n",
    "\n",
    "Example output:\n",
    "```\n",
    "drink:5\n",
    "\t1: 4\n",
    "\t2: 2,3,4\n",
    "\t3: 3\n",
    "\t4: 3\n",
    "\t5: 3\n",
    "ink:3\n",
    "\t3: 4\n",
    "\t4: 1\n",
    "\t5: 5\n",
    "like:5\n",
    "\t1: 1,3\n",
    "\t2: 1\n",
    "\t3: 2\n",
    "\t4: 2\n",
    "\t5: 1\n",
    "pink:2\n",
    "\t4: 4\n",
    "\t5: 4\n",
    "thing:1\n",
    "\t3: 1\n",
    "wink:2\n",
    "\t1: 2\n",
    "\t5: 2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1974b60a",
   "metadata": {},
   "source": [
    "If it's helpful you can use these classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3db7b0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TermPositions represents the positions of a term in a document\n",
    "# along with the term frequency in that document\n",
    "class TermPositions():\n",
    "    def __init__(self, positions):\n",
    "        self.positions = positions\n",
    "        self.freq = len(positions)\n",
    "\n",
    "# IndexTerm is a python dict holding the frequency of each term in\n",
    "# the index with an extra property containing the document frequency\n",
    "class IndexTerm(dict):\n",
    "    def __init__(self, iterable):\n",
    "        super().__init__(iterable)\n",
    "        self.docFreq = 0\n",
    "\n",
    "# Index is a python dict with an extra property containing the document count\n",
    "class Index(dict):\n",
    "    def __init__(self, iterable):\n",
    "        super().__init__(iterable)\n",
    "        self.docCount = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ec27cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index(dataset_path):\n",
    "    # Loop over the dataset file, reading out each document and its Id\n",
    "    \n",
    "    # Return the index\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "41c6756a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building index ...\n",
      "Done\n",
      "drink:5\n",
      "\t1: 4\n",
      "\t2: 2,3,4\n",
      "\t3: 3\n",
      "\t4: 3\n",
      "\t5: 3\n",
      "ink:3\n",
      "\t3: 4\n",
      "\t4: 1\n",
      "\t5: 5\n",
      "like:5\n",
      "\t1: 1,3\n",
      "\t2: 1\n",
      "\t3: 2\n",
      "\t4: 2\n",
      "\t5: 1\n",
      "pink:2\n",
      "\t4: 4\n",
      "\t5: 4\n",
      "thing:1\n",
      "\t3: 1\n",
      "wink:2\n",
      "\t1: 2\n",
      "\t5: 2\n"
     ]
    }
   ],
   "source": [
    "# Answer\n",
    "# Create index for the given collection XML file\n",
    "def create_index(collectionFile):\n",
    "    # Initialize empty index\n",
    "    index = Index({})\n",
    "    docId = 0\n",
    "    posIdx = 0\n",
    "\n",
    "    # Parse the XML file line by line and populate the index\n",
    "    f = open(collectionFile)\n",
    "    line = f.readline()\n",
    "    while line:\n",
    "        # Parse document number\n",
    "        docNumMatch = re.match('\\s*<docno>(\\d+)</docno>', line, re.IGNORECASE)\n",
    "        if docNumMatch:\n",
    "            docId = int(docNumMatch.group(1))\n",
    "            \n",
    "            # Increment the document count\n",
    "            index.docCount += 1\n",
    "\n",
    "            # Processing a new document so reset the position index\n",
    "            posIdx = 0\n",
    "\n",
    "        # Parse document text or document headline\n",
    "        elif re.match('\\s*<(text|headline)>', line, re.IGNORECASE):\n",
    "            # Process all the lines in the headline/text\n",
    "            line = f.readline()\n",
    "            while not re.match('\\s*</(text|headline)>', line, re.IGNORECASE):\n",
    "                # Preprocess the document text\n",
    "                terms = preprocess_line_en(line)\n",
    "            \n",
    "                # Add each term to the index\n",
    "                for term in terms:\n",
    "                    # Increment the term position\n",
    "                    posIdx += 1\n",
    "                    if term in index:\n",
    "                        if docId in index[term]:\n",
    "                            # If already seen this term in this doc append to the list of positions\n",
    "                            index[term][docId].positions.append(posIdx)\n",
    "                            index[term][docId].freq += 1\n",
    "                        else:\n",
    "                            # Otherwise create a new entry for this doc under the term\n",
    "                            index[term][docId] = TermPositions([posIdx])\n",
    "                    else:\n",
    "                        # Haven't seen this term before so add to the index\n",
    "                        index[term] = IndexTerm({docId: TermPositions([posIdx])})\n",
    "\n",
    "                line = f.readline()\n",
    "        line = f.readline()\n",
    "\n",
    "    # Calculate and store the document frequencies\n",
    "    for term in index:\n",
    "        index[term].docFreq = len(index[term])\n",
    "    \n",
    "    # Close the file\n",
    "    f.close()\n",
    "\n",
    "    return index\n",
    "\n",
    "print(\"Building index ...\")\n",
    "index = create_index('../datasets/example_search_dataset.xml')\n",
    "print(\"Done\")\n",
    "\n",
    "def print_index(index):\n",
    "# Print the contents of the index\n",
    "    for term in sorted(index):\n",
    "        print(f\"{term}:{index[term].docFreq}\")\n",
    "        for doc in index[term]:\n",
    "            print(f\"\\t{doc}: {','.join([str(x) for x in sorted(index[term][doc].positions)])}\")\n",
    "\n",
    "print_index(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a38234d",
   "metadata": {},
   "source": [
    "## Exercise 2 - Querying\n",
    "\n",
    "Now write a simple function to run a query: given a single word print out the list of documents containing that word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32a99e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer\n",
    "def query_doc_ids(index, term):\n",
    "    # Query the index for the document ids of documents the given term\n",
    "    # appears in\n",
    "    if term and term in index:\n",
    "        return index[term]\n",
    "    else:\n",
    "        return []\n",
    "    \n",
    "def single_word_search(word):\n",
    "    term = preprocess_line_en(word)[0]\n",
    "    results = query_doc_ids(index, term)\n",
    "    print(sorted(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "617f458d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 5]\n"
     ]
    }
   ],
   "source": [
    "single_word_search(\"pink\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a1f138",
   "metadata": {},
   "source": [
    "## Exercise 3 - Boolean Search\n",
    "\n",
    "This is much more advanced :)\n",
    "\n",
    "1. Write a function to process boolean queries: AND, OR and NOT. e.g. pink AND ink\n",
    "2. Implement phrase search\n",
    "3. Implement proximity search\n",
    "\n",
    "Test it with queries like \"wink AND pink\", \"wink and INK\", etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67d5cace",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (613922379.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[13], line 3\u001b[0;36m\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "def run_boolean_query(query):\n",
    "    # Run a boolean query on the index and print out the results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b0431fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer\n",
    "def run_boolean_query(index, query):\n",
    "    results = []\n",
    "\n",
    "    # Parse the query\n",
    "    queryProximity = re.match(\"#(\\d+)\\((\\w+),\\s*(\\w+)\\)\", query)\n",
    "    queryPhrase = re.match(\"\\\"(.+)\\\"\", query)\n",
    "    queryAndNot = re.match(\"(.+) AND NOT (.+)\", query)\n",
    "    queryAnd = re.match(\"(.+) AND (.+)\", query)\n",
    "    queryOrNot = re.match(\"(.+) OR NOT (.+)\", query)\n",
    "    queryOr = re.match(\"(.+) OR (.+)\", query)\n",
    "    querySingleTerm = re.match(\"(\\w+)$\", query)\n",
    "\n",
    "    if queryAndNot:\n",
    "        results1 = run_boolean_query(index, queryAndNot.group(1))\n",
    "        result2 = run_boolean_query(index, queryAndNot.group(2))\n",
    "        results = [id for id in results1 if id not in result2]\n",
    "\n",
    "    elif queryAnd:\n",
    "        results1 = run_boolean_query(index, queryAnd.group(1))\n",
    "        result2 = run_boolean_query(index, queryAnd.group(2))\n",
    "        results = [id for id in results1 if id in result2]\n",
    "\n",
    "    elif queryOrNot:\n",
    "        term1 = preprocess_line_en(queryOrNot.group(1))[0]\n",
    "        term2 = preprocess_line_en(queryOrNot.group(2))[0]\n",
    "        results1 = query_doc_ids(index, term1)\n",
    "        # Run through every other term in the index to find doc ids \n",
    "        # that do not contain term2. This is super expensive as it \n",
    "        # runs through the whole index. \n",
    "        # Is there a better way to do this?\n",
    "        term2DocIds = query_doc_ids(index, term2)\n",
    "        results2 = [docId for term in index if term != term2 for docId in index[term] if docId not in term2DocIds]\n",
    "        results = set(results1 + results2)\n",
    "\n",
    "    elif queryOr:\n",
    "        results1 = run_boolean_query(index, queryOr.group(1))\n",
    "        result2 = run_boolean_query(index, queryOr.group(2))\n",
    "        results = set(results1 + result2)\n",
    "\n",
    "    elif queryPhrase:\n",
    "        terms = preprocess_line_en(queryPhrase.group(1))\n",
    "        results = query_doc_ids(index, terms[0])\n",
    "        resultPrev = results\n",
    "        # Loop over each pair of adjacent terms and find doc ids \n",
    "        # where the terms are in phrase order and take the intersection\n",
    "        for i in range(1, len(terms)):\n",
    "            resultNext = query_doc_ids(index, terms[i])\n",
    "            resultCurTerms = [id for id in resultPrev if id in resultNext and positionsInPhraseOrder(index[terms[i-1]][id], index[terms[i]][id])]\n",
    "            results = [docId for docId in results if docId in resultCurTerms]\n",
    "            resultPrev = resultNext\n",
    "\n",
    "    elif queryProximity:\n",
    "        proximity = int(queryProximity.group(1))\n",
    "        term1 = preprocess_line_en(queryProximity.group(2))[0]\n",
    "        term2 = preprocess_line_en(queryProximity.group(3))[0]\n",
    "        results1 = query_doc_ids(index, term1)\n",
    "        result2 = query_doc_ids(index, term2)\n",
    "        results = [id for id in results1 if id in result2 and posiitonsInProximity(index[term1][id], index[term2][id], proximity)]\n",
    "\n",
    "    elif querySingleTerm:\n",
    "        term = preprocess_line_en(querySingleTerm.group(1))[0]\n",
    "        results = query_doc_ids(index, term)\n",
    "\n",
    "    else:\n",
    "        print(f\"Unable to parse query: {query}\")\n",
    "    \n",
    "    return sorted(results)\n",
    "\n",
    "def run_query(query):\n",
    "    results = run_boolean_query(index, query)\n",
    "    print(results)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7e0eb62a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 5]\n"
     ]
    }
   ],
   "source": [
    "run_query(\"pink AND ink\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3db0490",
   "metadata": {},
   "source": [
    "## Exercise 4 - Running on TREC dataset\n",
    "\n",
    "1. Now build and run queries against your inverted index against the TREC dataset: `datasets/trec.5000.xml`\n",
    "2. Test with more complex boolean queries\n",
    "3. Verify the results by opening up the dataset file in VSCode and checking the documents contain the search words!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b60159d",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = create_index('../datasets/trec.5000.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5290563b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5106, 5128, 5470, 5472, 5474, 5482, 5492, 5507, 5508, 5966, 6050, 6051, 6286, 6439, 6447, 6451, 6456, 6586, 6800, 6903, 6936, 7570, 7595, 7662, 7671, 7677, 7683, 7699, 7708, 7709, 7712, 7713, 7787, 7865, 8057, 8316, 8322, 8324, 8449, 8450, 8451, 8523, 8608, 8741, 8795, 8804, 8805, 8817, 8818, 8822, 8824, 8911, 8912, 8913, 8929, 9035, 9051, 9052, 9053, 9344, 9396, 9433, 9563, 9718, 9793, 10000]\n"
     ]
    }
   ],
   "source": [
    "run_query(\"fish\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
