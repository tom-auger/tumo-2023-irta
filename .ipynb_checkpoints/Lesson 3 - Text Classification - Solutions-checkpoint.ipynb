{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "051d8886",
   "metadata": {},
   "source": [
    "# Lesson 3: Text Classification\n",
    "\n",
    "Text classification is the process of sorting documents of text into a set of predefined categories. \n",
    "\n",
    "In this lab we'll train machine learning models to classify the sentiment of Twitter tweets as either **positive, negative, or neutral**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "865043f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from collections import Counter\n",
    "import re\n",
    "import string\n",
    "from scipy import sparse \n",
    "import sklearn\n",
    "from sklearn import svm, ensemble, naive_bayes, linear_model\n",
    "import numpy as np\n",
    "import Stemmer\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f00ba0",
   "metadata": {},
   "source": [
    "## Inspect the dataset\n",
    "\n",
    "Each line of the file `tweets_train.txt` contains a tweet and its sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01f7c18d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "640169120600862720\tneutral\tHaruna Lukmon may av just played himself out of d Super Eagles under coach Sunday Oliseh\n",
      "\n",
      "635946254254624769\tneutral\tZach Putnam will be unavailable for the White Sox again tonight after experiencing right groin soreness while warming up Saturday night.\n",
      "\n",
      "667121920333258752\tnegative\t\"\"\"\"\"\"\"@daithimckay what about the victims of IRA terrorism David, do you ever think of them? Kingsmill, Le Mon or even Kevin McGuigan\"\"\"\"\"\"\"\n",
      "\n",
      "628637519643586560\tneutral\t\"\"\"\"\"\"\"LHP Matt Boyd, traded to @tigers in David Price deal, will start tomorrow vs. the @Royals. #BlueJays\"\"\"\"\"\"\"\n",
      "\n",
      "622623084395114496\tneutral\tWhat should I wear to the Taylor Swift concert tomorrow? What would my Bad Blood character name be? These questions will keep me up tonight.\n",
      "\n",
      "638281382184288256\tneutral\t\"\"\"\"\"\"\"@HighOnVibe Depends what she said though, Nicki may have had a reason to start! Have you seen the video?\"\"\"\"\"\"\"\n",
      "\n",
      "624067647529947136\tpositive\t\"\"\"\"\"\"\"Jake and I will be in Sharknado 4. We may die in the first scene, but we'll be in it. #HollywoodDreams\"\"\"\"\"\"\"\n",
      "\n",
      "805242572541198336\tpositive\tI wish I was Ashley Graham\n",
      "\n",
      "622795886507225088\tnegative\tPaul Dunne with a birdie putt on the 12th hole and just misses it. Very unlucky. Stays in a share of the lead at -11 with Spieth #TheOpen\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"datasets/tweets_train.txt\") as f:\n",
    "    for _ in range(2):\n",
    "        f.readline()\n",
    "    for _ in range(9):\n",
    "        print(f.readline())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8732772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The file 'datasets/stopwords_en.txt' contains a list of stopwords - one per line.\n",
    "with open(\"datasets/stopwords_en.txt\") as f:\n",
    "    enStopWords = set(f.read().splitlines())\n",
    "\n",
    "# Initialze the SnowballStemmer\n",
    "enStemmer = Stemmer.Stemmer('english')\n",
    "\n",
    "def preprocess_line_en(line: str) -> list[str]:\n",
    "    # Convert to lower case\n",
    "    tokens = line.lower()\n",
    "    \n",
    "    # Split into tokens with no punctuation\n",
    "    tokens = re.split(\"[^\\w]\", tokens)\n",
    "    \n",
    "    # Remove empty strings and stop words and apply the stemmer\n",
    "    # Initially disable stopping and stemming\n",
    "    #tokens = [enStemmer.stemWord(x) for x in tokens if x and x not in enStopWords]\n",
    "    \n",
    "    # Return the tokens\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "855ca6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(lines):\n",
    "    # Preprocess data for classification\n",
    "    documents = []\n",
    "    categories = []\n",
    "    vocab = set()\n",
    "    \n",
    "    # url_regex = re.compile(r'https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)')\n",
    "        \n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            tweet_id, category, tweet = line.split('\\t')\n",
    "\n",
    "            # tweet = url_regex.sub('', tweet)\n",
    "                \n",
    "            # Preprocess the text\n",
    "            terms = preprocess_line_en(tweet)\n",
    "\n",
    "            # Add terms to the vocab\n",
    "            for term in terms:\n",
    "                vocab.add(term)\n",
    "           \n",
    "            # Add document to the collection of docs\n",
    "            documents.append(terms)\n",
    "\n",
    "            # Add category to the list of categories\n",
    "            categories.append(category)\n",
    "    \n",
    "    return documents, categories, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c55a946",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset(train_test):\n",
    "        with open(f\"datasets/tweets_{train_test}.txt\", encoding=\"latin-1\") as f:\n",
    "            # Skip the header \n",
    "            return f.readlines()[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb5625f",
   "metadata": {},
   "source": [
    "Load the dataset, splitting it into a 90-10% train/validation test set split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1e2baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_val_data(seed):\n",
    "    # Load the corpus\n",
    "\n",
    "    # Randomly permute the corpus\n",
    "    \n",
    "    # Split the data into 90% training set 10% development set\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1845444f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer      \n",
    "def get_train_val_data(seed):\n",
    "    # Load the corpus\n",
    "    data_lines = read_dataset(\"train\")\n",
    "\n",
    "    # Randomly permute the corpus\n",
    "    rnd = np.random.RandomState(seed=seed)\n",
    "    rnd.shuffle(data_lines)\n",
    "\n",
    "    # Split the data into 90% training set 10% development set\n",
    "    split_idx = int(len(data_lines) * 0.9)\n",
    "    train_data, val_data = data_lines[:split_idx], data_lines[split_idx:]\n",
    "    return train_data, val_data\n",
    "        \n",
    "seed = 1234\n",
    "training_data, val_data = get_train_val_data(seed)\n",
    "test_data = read_dataset(\"test\")\n",
    "\n",
    "train_docs, train_categories, train_vocab = preprocess_dataset(training_data)\n",
    "val_docs, val_categories, val_vocab = preprocess_dataset(val_data)\n",
    "test_docs, test_categories, test_vocab = preprocess_dataset(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4433f83",
   "metadata": {},
   "source": [
    "1. Print the number of documents and size of vocabulary in each dataset.\n",
    "2. Print the distribution of categories in the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6392e28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print some info about the dataset\n",
    "\n",
    "\n",
    "# Print the distribution of categories in the training set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "327b2293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset has 16781 documents and vocab size 36791\n",
      "Validation dataset has 1865 documents and vocab size 8221\n",
      "Test dataset has 4662 documents and vocab size 15268\n",
      "\n",
      "Distribution of categories:\n",
      "[('neutral', 7915), ('positive', 5354), ('negative', 3512)]\n"
     ]
    }
   ],
   "source": [
    "#Â Answer\n",
    "# Print some info about the dataset\n",
    "print(f\"Training dataset has {len(train_docs)} documents and vocab size {len(train_vocab)}\")\n",
    "print(f\"Validation dataset has {len(val_docs)} documents and vocab size {len(val_vocab)}\")\n",
    "print(f\"Test dataset has {len(test_docs)} documents and vocab size {len(test_vocab)}\")\n",
    "\n",
    "# Print the distribution of categories in the training set\n",
    "print()\n",
    "print(\"Distribution of categories:\")\n",
    "print(collections.Counter(train_categories).most_common())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8403dbab",
   "metadata": {},
   "source": [
    "Convert the dataset into a bag-of-words representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6823b1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Every word in the vocabulary is given an index\n",
    "\n",
    "\n",
    "# Every category is given an index too\n",
    "\n",
    "\n",
    "# Convert to bag of words\n",
    "# This is a count matrix where each row is a document and each column is a word. \n",
    "# The elements of the matrix are the frequencies of how often that word appears in the dataset.\n",
    "def convert_to_bow_matrix(data, word2id):\n",
    "\n",
    "    # Matrix size is number of docs, vocab size + 1 (for OOV)\n",
    "\n",
    "    # Matrix indexed by [doc_id, token_id]\n",
    "    \n",
    "    # Loop over documents and insert word counts\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71065489",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Â Answer\n",
    "# Every word in the vocabulary is given an index\n",
    "word2id = {}\n",
    "for word_id, word in enumerate(train_vocab):\n",
    "    word2id[word] = word_id\n",
    "\n",
    "# Every category is given an index too\n",
    "cat2id = {}\n",
    "for cat_id, cat in enumerate(set(train_categories)):\n",
    "    cat2id[cat] = cat_id\n",
    "\n",
    "# Convert to bag of words\n",
    "# This is a count matrix where each row is a document and each column is a word. \n",
    "# The elements of the matrix are the frequencies of how often that word appears in the dataset.\n",
    "def convert_to_bow_matrix(data, word2id):\n",
    "\n",
    "    # Matrix size is number of docs, vocab size + 1 (for OOV)\n",
    "    matrix_size = (len(data), len(word2id) + 1)\n",
    "    oov_index = len(word2id)\n",
    "\n",
    "    # Matrix indexed by [doc_id, token_id]\n",
    "    X = sparse.dok_matrix(matrix_size)\n",
    "    \n",
    "    # Loop over documents and insert word counts\n",
    "    for doc_id, doc in enumerate(data):\n",
    "        for word in doc:\n",
    "            # default is 0. Add the count for this word in this doc\n",
    "            # if the word is oov, then increment the oov index\n",
    "            X[doc_id, word2id.get(word, oov_index)] += 1\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdb86ad",
   "metadata": {},
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2752af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b47c70f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(C=1000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(C=1000)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(C=1000)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Â Answer\n",
    "X_train = convert_to_bow_matrix(train_docs, word2id)\n",
    "y_train = [cat2id[cat] for cat in train_categories]\n",
    "\n",
    "# Train the model\n",
    "model = sklearn.svm.SVC(C=1000)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364dd3c1",
   "metadata": {},
   "source": [
    "Compute the accuracy of the model by evaluating predictions on the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f0fb27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(predictions, targets):\n",
    "    num_correct = 0\n",
    "    num_total = len(predictions)\n",
    "    for predicted, target in zip(predictions, targets):\n",
    "        if predicted == target:\n",
    "            num_correct += 1\n",
    "    return num_correct / num_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e75fbc70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy:  0.9994040879566176\n"
     ]
    }
   ],
   "source": [
    "y_train_predictions = model.predict(X_train)\n",
    "accuracy = compute_accuracy(y_train_predictions, y_train)\n",
    "print(\"Training accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ae25cc",
   "metadata": {},
   "source": [
    "Compute the accuracy of the model on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0c879b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy on the validation set\n",
    "X_val = convert_to_bow_matrix(val_docs, word2id)\n",
    "y_val = [cat2id[cat] for cat in val_categories]\n",
    "\n",
    "y_val_predictions = model.predict(X_val)\n",
    "accuracy = compute_accuracy(y_val_predictions, y_val)\n",
    "\n",
    "print(\"Validation accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cd4b53",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "Can we improve the performance of the model? Try some of the following ideas and experiment with any of your own:\n",
    "1. Modify preprocessing to remove hyperlinks\n",
    "2. Modify preprocessing to keep hashtags (don't remove the # character)\n",
    "3. Try using another model instead of SVM, for example `sklearn.linear_model.LogisticRegression` or `sklearn.ensemble.RandomForestClassifier`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290f7839",
   "metadata": {},
   "source": [
    "Once all optimizations to the model have been made evaluate it on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25b0228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and calculate accuracy on the test set\n",
    "# X_test = convert_to_bow_matrix(test_docs, word2id)\n",
    "# y_test = [cat2id[cat] for cat in test_categories]\n",
    "\n",
    "# y_test_predictions = model.predict(X_test)\n",
    "# accuracy = compute_accuracy(y_test_predictions, y_test)\n",
    "# print(\"Test accuracy: \", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
